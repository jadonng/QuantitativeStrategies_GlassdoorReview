{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook was run on Google Colab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2RicgFlLoPR9"
   },
   "source": [
    "### Mount Main Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 62272,
     "status": "ok",
     "timestamp": 1733930284440,
     "user": {
      "displayName": "Rhenald",
      "userId": "16257034015682689428"
     },
     "user_tz": -480
    },
    "id": "c3HXE4ryoPsB",
    "outputId": "e684dfd1-bca2-440e-97e5-d82e0f08d2c0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NdMEHnCzpUUj"
   },
   "source": [
    "### GDrive Mounting (PyDrive2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1733930284440,
     "user": {
      "displayName": "Rhenald",
      "userId": "16257034015682689428"
     },
     "user_tz": -480
    },
    "id": "Yn1_U_kgpUmG"
   },
   "outputs": [],
   "source": [
    "# !pip install PyDrive2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1733930284440,
     "user": {
      "displayName": "Rhenald",
      "userId": "16257034015682689428"
     },
     "user_tz": -480
    },
    "id": "mCv9e02wpag6"
   },
   "outputs": [],
   "source": [
    "# from pydrive2.auth import GoogleAuth\n",
    "# from pydrive2.drive import GoogleDrive\n",
    "# from google.colab import auth\n",
    "# from oauth2client.client import GoogleCredentials\n",
    "\n",
    "# # Authenticate and create the PyDrive client for acc2\n",
    "# auth.authenticate_user()\n",
    "# gauth = GoogleAuth()\n",
    "# gauth.credentials = GoogleCredentials.get_application_default()\n",
    "# drive2 = GoogleDrive(gauth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1733930284440,
     "user": {
      "displayName": "Rhenald",
      "userId": "16257034015682689428"
     },
     "user_tz": -480
    },
    "id": "owFk82Qcp4h-"
   },
   "outputs": [],
   "source": [
    "# # List files in a specific folder in the second Google Drive\n",
    "# folder_id = '1-K79qAXkFCL0KUhztx9YOWiLz5iNGOtk'  # Replace with the actual folder ID\n",
    "# file_list = drive2.ListFile({'q': f\"'{folder_id}' in parents and trashed=false\"}).GetList()\n",
    "\n",
    "# for file in file_list:\n",
    "#     print(f\"title: {file['title']}, id: {file['id']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bYMsVTvqo98W"
   },
   "source": [
    "### GDrive Mounting (GCP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1733930284440,
     "user": {
      "displayName": "Rhenald",
      "userId": "16257034015682689428"
     },
     "user_tz": -480
    },
    "id": "4Z1DuK2amgLd"
   },
   "outputs": [],
   "source": [
    "# !pip install google-auth google-auth-oauthlib google-auth-httplib2 google-api-python-client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1733930284440,
     "user": {
      "displayName": "Rhenald",
      "userId": "16257034015682689428"
     },
     "user_tz": -480
    },
    "id": "rtDcyzKInUT3"
   },
   "outputs": [],
   "source": [
    "# # Mounting other google drives and getting into the files seems to connect with GCP, which for now is not yet explored.\n",
    "\n",
    "# import os\n",
    "# import google.auth\n",
    "# from google.auth.transport.requests import Request\n",
    "# from google_auth_oauthlib.flow import InstalledAppFlow\n",
    "# from googleapiclient.discovery import build\n",
    "\n",
    "# # Path to the uploaded credentials file\n",
    "# creds_path = '/content/credentials.json'\n",
    "\n",
    "# # Define the scopes\n",
    "# SCOPES = ['https://www.googleapis.com/auth/drive']\n",
    "\n",
    "# # Authenticate and create the service\n",
    "# def authenticate_gdrive(creds_path):\n",
    "#     creds = None\n",
    "#     if os.path.exists('token.json'):\n",
    "#         creds = google.auth.load_credentials_from_file('token.json', SCOPES)\n",
    "#     if not creds or not creds.valid:\n",
    "#         if creds and creds.expired and creds.refresh_token:\n",
    "#             creds.refresh(Request())\n",
    "#         else:\n",
    "#             flow = InstalledAppFlow.from_client_secrets_file(creds_path, SCOPES)\n",
    "#             creds = flow.run_local_server(port=0)\n",
    "#         with open('token.json', 'w') as token:\n",
    "#             token.write(creds.to_json())\n",
    "#     return build('drive', 'v3', credentials=creds)\n",
    "\n",
    "# # Authenticate and create the Drive service\n",
    "# service = authenticate_gdrive(creds_path)\n",
    "\n",
    "# # Function to list files in a specific folder\n",
    "# def list_files_in_folder(service, folder_id):\n",
    "#     query = f\"'{folder_id}' in parents\"\n",
    "#     results = service.files().list(q=query, pageSize=10).execute()\n",
    "#     items = results.get('files', [])\n",
    "\n",
    "#     if not items:\n",
    "#         print('No files found.')\n",
    "#     else:\n",
    "#         print('Files:')\n",
    "#         for item in items:\n",
    "#             print(f\"{item['name']} ({item['id']})\")\n",
    "\n",
    "# # Replace with the actual folder ID of 'Colab Notebooks/FINA4359'\n",
    "# folder_id = 'your_folder_id_here'\n",
    "\n",
    "# # List files in the specified folder\n",
    "# list_files_in_folder(service, folder_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HKDLw3O4j1sV"
   },
   "source": [
    "### Download RAPIDSAI CUML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 155440,
     "status": "ok",
     "timestamp": 1733930439870,
     "user": {
      "displayName": "Rhenald",
      "userId": "16257034015682689428"
     },
     "user_tz": -480
    },
    "id": "Vs5Bbpe9j1_M",
    "outputId": "8e37a612-b809-494e-c518-7cd4a65946f3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'rapidsai-csp-utils'...\n",
      "remote: Enumerating objects: 553, done.\u001b[K\n",
      "remote: Counting objects: 100% (284/284), done.\u001b[K\n",
      "remote: Compressing objects: 100% (182/182), done.\u001b[K\n",
      "remote: Total 553 (delta 179), reused 147 (delta 100), pack-reused 269 (from 1)\u001b[K\n",
      "Receiving objects: 100% (553/553), 178.44 KiB | 1.65 MiB/s, done.\n",
      "Resolving deltas: 100% (281/281), done.\n",
      "Collecting pynvml\n",
      "  Downloading pynvml-12.0.0-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting nvidia-ml-py<13.0.0a0,>=12.0.0 (from pynvml)\n",
      "  Downloading nvidia_ml_py-12.560.30-py3-none-any.whl.metadata (8.6 kB)\n",
      "Downloading pynvml-12.0.0-py3-none-any.whl (26 kB)\n",
      "Downloading nvidia_ml_py-12.560.30-py3-none-any.whl (40 kB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 40.5/40.5 kB 3.0 MB/s eta 0:00:00\n",
      "Installing collected packages: nvidia-ml-py, pynvml\n",
      "Successfully installed nvidia-ml-py-12.560.30 pynvml-12.0.0\n",
      "Installing RAPIDS remaining 24.10.* libraries\n",
      "Looking in indexes: https://pypi.org/simple, https://pypi.nvidia.com\n",
      "Requirement already satisfied: cudf-cu12==24.10.* in /usr/local/lib/python3.10/dist-packages (24.10.1)\n",
      "Collecting cuml-cu12==24.10.*\n",
      "  Downloading https://pypi.nvidia.com/cuml-cu12/cuml_cu12-24.10.0-cp310-cp310-manylinux_2_28_x86_64.whl (567.7 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 567.7/567.7 MB 1.6 MB/s eta 0:00:00\n",
      "Collecting cugraph-cu12==24.10.*\n",
      "  Downloading https://pypi.nvidia.com/cugraph-cu12/cugraph_cu12-24.10.0-cp310-cp310-manylinux_2_28_x86_64.whl (1315.2 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.3/1.3 GB 914.8 kB/s eta 0:00:00\n",
      "Collecting cuspatial-cu12==24.10.*\n",
      "  Downloading https://pypi.nvidia.com/cuspatial-cu12/cuspatial_cu12-24.10.0-cp310-cp310-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (4.3 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 4.3/4.3 MB 92.7 MB/s eta 0:00:00\n",
      "Collecting cuproj-cu12==24.10.*\n",
      "  Downloading https://pypi.nvidia.com/cuproj-cu12/cuproj_cu12-24.10.0-cp310-cp310-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (915 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 915.5/915.5 kB 54.9 MB/s eta 0:00:00\n",
      "Collecting cuxfilter-cu12==24.10.*\n",
      "  Downloading https://pypi.nvidia.com/cuxfilter-cu12/cuxfilter_cu12-24.10.0-py3-none-any.whl (83 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 83.6/83.6 kB 7.9 MB/s eta 0:00:00\n",
      "Collecting cucim-cu12==24.10.*\n",
      "  Downloading https://pypi.nvidia.com/cucim-cu12/cucim_cu12-24.10.0-cp310-cp310-manylinux_2_28_x86_64.whl (5.6 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 5.6/5.6 MB 97.2 MB/s eta 0:00:00\n",
      "Requirement already satisfied: pylibraft-cu12==24.10.* in /usr/local/lib/python3.10/dist-packages (24.10.0)\n",
      "Collecting raft-dask-cu12==24.10.*\n",
      "  Downloading https://pypi.nvidia.com/raft-dask-cu12/raft_dask_cu12-24.10.0-cp310-cp310-manylinux_2_28_x86_64.whl (196.9 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 196.9/196.9 MB 7.0 MB/s eta 0:00:00\n",
      "Requirement already satisfied: nx-cugraph-cu12==24.10.* in /usr/local/lib/python3.10/dist-packages (24.10.0)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (3.11.9)\n",
      "Requirement already satisfied: cachetools in /usr/local/lib/python3.10/dist-packages (from cudf-cu12==24.10.*) (5.5.0)\n",
      "Requirement already satisfied: cuda-python<13.0a0,>=12.0 in /usr/local/lib/python3.10/dist-packages (from cudf-cu12==24.10.*) (12.2.1)\n",
      "Requirement already satisfied: cupy-cuda12x>=12.0.0 in /usr/local/lib/python3.10/dist-packages (from cudf-cu12==24.10.*) (12.2.0)\n",
      "Requirement already satisfied: fsspec>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from cudf-cu12==24.10.*) (2024.10.0)\n",
      "Requirement already satisfied: libcudf-cu12==24.10.* in /usr/local/lib/python3.10/dist-packages (from cudf-cu12==24.10.*) (24.10.1)\n",
      "Requirement already satisfied: numba>=0.57 in /usr/local/lib/python3.10/dist-packages (from cudf-cu12==24.10.*) (0.60.0)\n",
      "Requirement already satisfied: numpy<3.0a0,>=1.23 in /usr/local/lib/python3.10/dist-packages (from cudf-cu12==24.10.*) (1.26.4)\n",
      "Requirement already satisfied: nvtx>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from cudf-cu12==24.10.*) (0.2.10)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from cudf-cu12==24.10.*) (24.2)\n",
      "Requirement already satisfied: pandas<2.2.3dev0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from cudf-cu12==24.10.*) (2.2.2)\n",
      "Requirement already satisfied: pyarrow<18.0.0a0,>=14.0.0 in /usr/local/lib/python3.10/dist-packages (from cudf-cu12==24.10.*) (17.0.0)\n",
      "Requirement already satisfied: pylibcudf-cu12==24.10.* in /usr/local/lib/python3.10/dist-packages (from cudf-cu12==24.10.*) (24.10.1)\n",
      "Requirement already satisfied: pynvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from cudf-cu12==24.10.*) (0.4.0)\n",
      "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from cudf-cu12==24.10.*) (13.9.4)\n",
      "Requirement already satisfied: rmm-cu12==24.10.* in /usr/local/lib/python3.10/dist-packages (from cudf-cu12==24.10.*) (24.10.0)\n",
      "Requirement already satisfied: typing_extensions>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from cudf-cu12==24.10.*) (4.12.2)\n",
      "Collecting cuvs-cu12==24.10.* (from cuml-cu12==24.10.*)\n",
      "  Downloading https://pypi.nvidia.com/cuvs-cu12/cuvs_cu12-24.10.0-cp310-cp310-manylinux_2_28_x86_64.whl (836.6 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 836.6/836.6 MB 1.4 MB/s eta 0:00:00\n",
      "Collecting dask-cuda==24.10.* (from cuml-cu12==24.10.*)\n",
      "  Downloading https://pypi.nvidia.com/dask-cuda/dask_cuda-24.10.0-py3-none-any.whl (133 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 133.2/133.2 kB 11.7 MB/s eta 0:00:00\n",
      "Collecting dask-cudf-cu12==24.10.* (from cuml-cu12==24.10.*)\n",
      "  Downloading https://pypi.nvidia.com/dask-cudf-cu12/dask_cudf_cu12-24.10.1-py3-none-any.whl (56 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 56.3/56.3 kB 5.4 MB/s eta 0:00:00\n",
      "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.10/dist-packages (from cuml-cu12==24.10.*) (1.4.2)\n",
      "Requirement already satisfied: nvidia-cublas-cu12 in /usr/local/lib/python3.10/dist-packages (from cuml-cu12==24.10.*) (12.6.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12 in /usr/local/lib/python3.10/dist-packages (from cuml-cu12==24.10.*) (11.3.0.4)\n",
      "Requirement already satisfied: nvidia-curand-cu12 in /usr/local/lib/python3.10/dist-packages (from cuml-cu12==24.10.*) (10.3.7.77)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12 in /usr/local/lib/python3.10/dist-packages (from cuml-cu12==24.10.*) (11.7.1.2)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12 in /usr/local/lib/python3.10/dist-packages (from cuml-cu12==24.10.*) (12.5.4.2)\n",
      "Collecting rapids-dask-dependency==24.10.* (from cuml-cu12==24.10.*)\n",
      "  Downloading https://pypi.nvidia.com/rapids-dask-dependency/rapids_dask_dependency-24.10.0-py3-none-any.whl (15 kB)\n",
      "Requirement already satisfied: scipy>=1.8.0 in /usr/local/lib/python3.10/dist-packages (from cuml-cu12==24.10.*) (1.13.1)\n",
      "Collecting treelite==4.3.0 (from cuml-cu12==24.10.*)\n",
      "  Downloading treelite-4.3.0-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: pylibcugraph-cu12==24.10.* in /usr/local/lib/python3.10/dist-packages (from cugraph-cu12==24.10.*) (24.10.0)\n",
      "Collecting ucx-py-cu12==0.40.* (from cugraph-cu12==24.10.*)\n",
      "  Downloading https://pypi.nvidia.com/ucx-py-cu12/ucx_py_cu12-0.40.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_28_x86_64.whl (2.1 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.1/2.1 MB 79.0 MB/s eta 0:00:00\n",
      "Requirement already satisfied: geopandas>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from cuspatial-cu12==24.10.*) (1.0.1)\n",
      "Collecting libcuspatial-cu12==24.10.* (from cuspatial-cu12==24.10.*)\n",
      "  Downloading https://pypi.nvidia.com/libcuspatial-cu12/libcuspatial_cu12-24.10.0-py3-none-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (17.7 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 17.7/17.7 MB 90.3 MB/s eta 0:00:00\n",
      "Requirement already satisfied: bokeh>=3.1 in /usr/local/lib/python3.10/dist-packages (from cuxfilter-cu12==24.10.*) (3.6.2)\n",
      "Collecting datashader>=0.15 (from cuxfilter-cu12==24.10.*)\n",
      "  Downloading datashader-0.16.3-py2.py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: holoviews>=1.16.0 in /usr/local/lib/python3.10/dist-packages (from cuxfilter-cu12==24.10.*) (1.20.0)\n",
      "Collecting jupyter-server-proxy (from cuxfilter-cu12==24.10.*)\n",
      "  Downloading jupyter_server_proxy-4.4.0-py3-none-any.whl.metadata (8.7 kB)\n",
      "Requirement already satisfied: panel>=1.0 in /usr/local/lib/python3.10/dist-packages (from cuxfilter-cu12==24.10.*) (1.5.4)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from cucim-cu12==24.10.*) (8.1.7)\n",
      "Requirement already satisfied: lazy-loader>=0.1 in /usr/local/lib/python3.10/dist-packages (from cucim-cu12==24.10.*) (0.4)\n",
      "Requirement already satisfied: scikit-image<0.25.0a0,>=0.19.0 in /usr/local/lib/python3.10/dist-packages (from cucim-cu12==24.10.*) (0.24.0)\n",
      "Collecting distributed-ucxx-cu12==0.40.* (from raft-dask-cu12==24.10.*)\n",
      "  Downloading https://pypi.nvidia.com/distributed-ucxx-cu12/distributed_ucxx_cu12-0.40.0-py3-none-any.whl (24 kB)\n",
      "Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.10/dist-packages (from nx-cugraph-cu12==24.10.*) (3.4.2)\n",
      "Collecting pynvml<11.5,>=11.0.0 (from dask-cuda==24.10.*->cuml-cu12==24.10.*)\n",
      "  Downloading pynvml-11.4.1-py3-none-any.whl.metadata (7.7 kB)\n",
      "Collecting zict>=2.0.0 (from dask-cuda==24.10.*->cuml-cu12==24.10.*)\n",
      "  Downloading zict-3.0.0-py2.py3-none-any.whl.metadata (899 bytes)\n",
      "Collecting ucxx-cu12==0.40.* (from distributed-ucxx-cu12==0.40.*->raft-dask-cu12==24.10.*)\n",
      "  Downloading https://pypi.nvidia.com/ucxx-cu12/ucxx_cu12-0.40.0-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (722 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 722.7/722.7 kB 52.2 MB/s eta 0:00:00\n",
      "Collecting dask==2024.9.0 (from rapids-dask-dependency==24.10.*->cuml-cu12==24.10.*)\n",
      "  Downloading dask-2024.9.0-py3-none-any.whl.metadata (3.7 kB)\n",
      "Collecting distributed==2024.9.0 (from rapids-dask-dependency==24.10.*->cuml-cu12==24.10.*)\n",
      "  Downloading distributed-2024.9.0-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting dask-expr==1.1.14 (from rapids-dask-dependency==24.10.*->cuml-cu12==24.10.*)\n",
      "  Downloading dask_expr-1.1.14-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting libucx-cu12<1.18,>=1.15.0 (from ucx-py-cu12==0.40.*->cugraph-cu12==24.10.*)\n",
      "  Downloading libucx_cu12-1.17.0.post1-py3-none-manylinux_2_28_x86_64.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: cloudpickle>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from dask==2024.9.0->rapids-dask-dependency==24.10.*->cuml-cu12==24.10.*) (3.1.0)\n",
      "Requirement already satisfied: partd>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from dask==2024.9.0->rapids-dask-dependency==24.10.*->cuml-cu12==24.10.*) (1.4.2)\n",
      "Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.10/dist-packages (from dask==2024.9.0->rapids-dask-dependency==24.10.*->cuml-cu12==24.10.*) (6.0.2)\n",
      "Requirement already satisfied: toolz>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from dask==2024.9.0->rapids-dask-dependency==24.10.*->cuml-cu12==24.10.*) (0.12.1)\n",
      "Requirement already satisfied: importlib-metadata>=4.13.0 in /usr/local/lib/python3.10/dist-packages (from dask==2024.9.0->rapids-dask-dependency==24.10.*->cuml-cu12==24.10.*) (8.5.0)\n",
      "Requirement already satisfied: jinja2>=2.10.3 in /usr/local/lib/python3.10/dist-packages (from distributed==2024.9.0->rapids-dask-dependency==24.10.*->cuml-cu12==24.10.*) (3.1.4)\n",
      "Requirement already satisfied: locket>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from distributed==2024.9.0->rapids-dask-dependency==24.10.*->cuml-cu12==24.10.*) (1.0.0)\n",
      "Requirement already satisfied: msgpack>=1.0.2 in /usr/local/lib/python3.10/dist-packages (from distributed==2024.9.0->rapids-dask-dependency==24.10.*->cuml-cu12==24.10.*) (1.1.0)\n",
      "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.10/dist-packages (from distributed==2024.9.0->rapids-dask-dependency==24.10.*->cuml-cu12==24.10.*) (5.9.5)\n",
      "Collecting sortedcontainers>=2.0.5 (from distributed==2024.9.0->rapids-dask-dependency==24.10.*->cuml-cu12==24.10.*)\n",
      "  Downloading sortedcontainers-2.4.0-py2.py3-none-any.whl.metadata (10 kB)\n",
      "Collecting tblib>=1.6.0 (from distributed==2024.9.0->rapids-dask-dependency==24.10.*->cuml-cu12==24.10.*)\n",
      "  Downloading tblib-3.0.0-py3-none-any.whl.metadata (25 kB)\n",
      "Requirement already satisfied: tornado>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from distributed==2024.9.0->rapids-dask-dependency==24.10.*->cuml-cu12==24.10.*) (6.3.3)\n",
      "Requirement already satisfied: urllib3>=1.26.5 in /usr/local/lib/python3.10/dist-packages (from distributed==2024.9.0->rapids-dask-dependency==24.10.*->cuml-cu12==24.10.*) (2.2.3)\n",
      "Collecting libucxx-cu12==0.40.* (from ucxx-cu12==0.40.*->distributed-ucxx-cu12==0.40.*->raft-dask-cu12==24.10.*)\n",
      "  Downloading https://pypi.nvidia.com/libucxx-cu12/libucxx_cu12-0.40.0-py3-none-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (511 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 511.7/511.7 kB 36.3 MB/s eta 0:00:00\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp) (2.4.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp) (1.3.1)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp) (4.0.3)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp) (0.2.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp) (1.18.3)\n",
      "Requirement already satisfied: contourpy>=1.2 in /usr/local/lib/python3.10/dist-packages (from bokeh>=3.1->cuxfilter-cu12==24.10.*) (1.3.1)\n",
      "Requirement already satisfied: pillow>=7.1.0 in /usr/local/lib/python3.10/dist-packages (from bokeh>=3.1->cuxfilter-cu12==24.10.*) (11.0.0)\n",
      "Requirement already satisfied: xyzservices>=2021.09.1 in /usr/local/lib/python3.10/dist-packages (from bokeh>=3.1->cuxfilter-cu12==24.10.*) (2024.9.0)\n",
      "Requirement already satisfied: cython in /usr/local/lib/python3.10/dist-packages (from cuda-python<13.0a0,>=12.0->cudf-cu12==24.10.*) (3.0.11)\n",
      "Requirement already satisfied: fastrlock>=0.5 in /usr/local/lib/python3.10/dist-packages (from cupy-cuda12x>=12.0.0->cudf-cu12==24.10.*) (0.8.2)\n",
      "Requirement already satisfied: colorcet in /usr/local/lib/python3.10/dist-packages (from datashader>=0.15->cuxfilter-cu12==24.10.*) (3.1.0)\n",
      "Requirement already satisfied: multipledispatch in /usr/local/lib/python3.10/dist-packages (from datashader>=0.15->cuxfilter-cu12==24.10.*) (1.0.0)\n",
      "Requirement already satisfied: param in /usr/local/lib/python3.10/dist-packages (from datashader>=0.15->cuxfilter-cu12==24.10.*) (2.1.1)\n",
      "Collecting pyct (from datashader>=0.15->cuxfilter-cu12==24.10.*)\n",
      "  Downloading pyct-0.5.0-py2.py3-none-any.whl.metadata (7.4 kB)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from datashader>=0.15->cuxfilter-cu12==24.10.*) (2.32.3)\n",
      "Requirement already satisfied: xarray in /usr/local/lib/python3.10/dist-packages (from datashader>=0.15->cuxfilter-cu12==24.10.*) (2024.10.0)\n",
      "Requirement already satisfied: pyogrio>=0.7.2 in /usr/local/lib/python3.10/dist-packages (from geopandas>=1.0.0->cuspatial-cu12==24.10.*) (0.10.0)\n",
      "Requirement already satisfied: pyproj>=3.3.0 in /usr/local/lib/python3.10/dist-packages (from geopandas>=1.0.0->cuspatial-cu12==24.10.*) (3.7.0)\n",
      "Requirement already satisfied: shapely>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from geopandas>=1.0.0->cuspatial-cu12==24.10.*) (2.0.6)\n",
      "Requirement already satisfied: pyviz-comms>=2.1 in /usr/local/lib/python3.10/dist-packages (from holoviews>=1.16.0->cuxfilter-cu12==24.10.*) (3.0.3)\n",
      "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba>=0.57->cudf-cu12==24.10.*) (0.43.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas<2.2.3dev0,>=2.0->cudf-cu12==24.10.*) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<2.2.3dev0,>=2.0->cudf-cu12==24.10.*) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas<2.2.3dev0,>=2.0->cudf-cu12==24.10.*) (2024.2)\n",
      "Requirement already satisfied: bleach in /usr/local/lib/python3.10/dist-packages (from panel>=1.0->cuxfilter-cu12==24.10.*) (6.2.0)\n",
      "Requirement already satisfied: linkify-it-py in /usr/local/lib/python3.10/dist-packages (from panel>=1.0->cuxfilter-cu12==24.10.*) (2.0.3)\n",
      "Requirement already satisfied: markdown in /usr/local/lib/python3.10/dist-packages (from panel>=1.0->cuxfilter-cu12==24.10.*) (3.7)\n",
      "Requirement already satisfied: markdown-it-py in /usr/local/lib/python3.10/dist-packages (from panel>=1.0->cuxfilter-cu12==24.10.*) (3.0.0)\n",
      "Requirement already satisfied: mdit-py-plugins in /usr/local/lib/python3.10/dist-packages (from panel>=1.0->cuxfilter-cu12==24.10.*) (0.4.2)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from panel>=1.0->cuxfilter-cu12==24.10.*) (4.66.6)\n",
      "Requirement already satisfied: imageio>=2.33 in /usr/local/lib/python3.10/dist-packages (from scikit-image<0.25.0a0,>=0.19.0->cucim-cu12==24.10.*) (2.36.1)\n",
      "Requirement already satisfied: tifffile>=2022.8.12 in /usr/local/lib/python3.10/dist-packages (from scikit-image<0.25.0a0,>=0.19.0->cucim-cu12==24.10.*) (2024.9.20)\n",
      "Requirement already satisfied: idna>=2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.17.0->aiohttp) (3.10)\n",
      "Requirement already satisfied: jupyter-server>=1.24.0 in /usr/local/lib/python3.10/dist-packages (from jupyter-server-proxy->cuxfilter-cu12==24.10.*) (1.24.0)\n",
      "Collecting simpervisor>=1.0.0 (from jupyter-server-proxy->cuxfilter-cu12==24.10.*)\n",
      "  Downloading simpervisor-1.0.0-py3-none-any.whl.metadata (4.3 kB)\n",
      "Requirement already satisfied: traitlets>=5.1.0 in /usr/local/lib/python3.10/dist-packages (from jupyter-server-proxy->cuxfilter-cu12==24.10.*) (5.7.1)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cufft-cu12->cuml-cu12==24.10.*) (12.6.85)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->cudf-cu12==24.10.*) (2.18.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2>=2.10.3->distributed==2024.9.0->rapids-dask-dependency==24.10.*->cuml-cu12==24.10.*) (3.0.2)\n",
      "Requirement already satisfied: anyio<4,>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from jupyter-server>=1.24.0->jupyter-server-proxy->cuxfilter-cu12==24.10.*) (3.7.1)\n",
      "Requirement already satisfied: argon2-cffi in /usr/local/lib/python3.10/dist-packages (from jupyter-server>=1.24.0->jupyter-server-proxy->cuxfilter-cu12==24.10.*) (23.1.0)\n",
      "Requirement already satisfied: jupyter-client>=6.1.12 in /usr/local/lib/python3.10/dist-packages (from jupyter-server>=1.24.0->jupyter-server-proxy->cuxfilter-cu12==24.10.*) (6.1.12)\n",
      "Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in /usr/local/lib/python3.10/dist-packages (from jupyter-server>=1.24.0->jupyter-server-proxy->cuxfilter-cu12==24.10.*) (5.7.2)\n",
      "Requirement already satisfied: nbconvert>=6.4.4 in /usr/local/lib/python3.10/dist-packages (from jupyter-server>=1.24.0->jupyter-server-proxy->cuxfilter-cu12==24.10.*) (7.16.4)\n",
      "Requirement already satisfied: nbformat>=5.2.0 in /usr/local/lib/python3.10/dist-packages (from jupyter-server>=1.24.0->jupyter-server-proxy->cuxfilter-cu12==24.10.*) (5.10.4)\n",
      "Requirement already satisfied: prometheus-client in /usr/local/lib/python3.10/dist-packages (from jupyter-server>=1.24.0->jupyter-server-proxy->cuxfilter-cu12==24.10.*) (0.21.1)\n",
      "Requirement already satisfied: pyzmq>=17 in /usr/local/lib/python3.10/dist-packages (from jupyter-server>=1.24.0->jupyter-server-proxy->cuxfilter-cu12==24.10.*) (24.0.1)\n",
      "Requirement already satisfied: Send2Trash in /usr/local/lib/python3.10/dist-packages (from jupyter-server>=1.24.0->jupyter-server-proxy->cuxfilter-cu12==24.10.*) (1.8.3)\n",
      "Requirement already satisfied: terminado>=0.8.3 in /usr/local/lib/python3.10/dist-packages (from jupyter-server>=1.24.0->jupyter-server-proxy->cuxfilter-cu12==24.10.*) (0.18.1)\n",
      "Requirement already satisfied: websocket-client in /usr/local/lib/python3.10/dist-packages (from jupyter-server>=1.24.0->jupyter-server-proxy->cuxfilter-cu12==24.10.*) (1.8.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py->panel>=1.0->cuxfilter-cu12==24.10.*) (0.1.2)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from pyogrio>=0.7.2->geopandas>=1.0.0->cuspatial-cu12==24.10.*) (2024.8.30)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas<2.2.3dev0,>=2.0->cudf-cu12==24.10.*) (1.16.0)\n",
      "Requirement already satisfied: webencodings in /usr/local/lib/python3.10/dist-packages (from bleach->panel>=1.0->cuxfilter-cu12==24.10.*) (0.5.1)\n",
      "Requirement already satisfied: uc-micro-py in /usr/local/lib/python3.10/dist-packages (from linkify-it-py->panel>=1.0->cuxfilter-cu12==24.10.*) (1.0.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->datashader>=0.15->cuxfilter-cu12==24.10.*) (3.4.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.1.0->jupyter-server>=1.24.0->jupyter-server-proxy->cuxfilter-cu12==24.10.*) (1.3.1)\n",
      "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.1.0->jupyter-server>=1.24.0->jupyter-server-proxy->cuxfilter-cu12==24.10.*) (1.2.2)\n",
      "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata>=4.13.0->dask==2024.9.0->rapids-dask-dependency==24.10.*->cuml-cu12==24.10.*) (3.21.0)\n",
      "Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.10/dist-packages (from jupyter-core!=5.0.*,>=4.12->jupyter-server>=1.24.0->jupyter-server-proxy->cuxfilter-cu12==24.10.*) (4.3.6)\n",
      "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=6.4.4->jupyter-server>=1.24.0->jupyter-server-proxy->cuxfilter-cu12==24.10.*) (4.12.3)\n",
      "Requirement already satisfied: defusedxml in /usr/local/lib/python3.10/dist-packages (from nbconvert>=6.4.4->jupyter-server>=1.24.0->jupyter-server-proxy->cuxfilter-cu12==24.10.*) (0.7.1)\n",
      "Requirement already satisfied: jupyterlab-pygments in /usr/local/lib/python3.10/dist-packages (from nbconvert>=6.4.4->jupyter-server>=1.24.0->jupyter-server-proxy->cuxfilter-cu12==24.10.*) (0.3.0)\n",
      "Requirement already satisfied: mistune<4,>=2.0.3 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=6.4.4->jupyter-server>=1.24.0->jupyter-server-proxy->cuxfilter-cu12==24.10.*) (3.0.2)\n",
      "Requirement already satisfied: nbclient>=0.5.0 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=6.4.4->jupyter-server>=1.24.0->jupyter-server-proxy->cuxfilter-cu12==24.10.*) (0.10.1)\n",
      "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=6.4.4->jupyter-server>=1.24.0->jupyter-server-proxy->cuxfilter-cu12==24.10.*) (1.5.1)\n",
      "Requirement already satisfied: tinycss2 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=6.4.4->jupyter-server>=1.24.0->jupyter-server-proxy->cuxfilter-cu12==24.10.*) (1.4.0)\n",
      "Requirement already satisfied: fastjsonschema>=2.15 in /usr/local/lib/python3.10/dist-packages (from nbformat>=5.2.0->jupyter-server>=1.24.0->jupyter-server-proxy->cuxfilter-cu12==24.10.*) (2.21.1)\n",
      "Requirement already satisfied: jsonschema>=2.6 in /usr/local/lib/python3.10/dist-packages (from nbformat>=5.2.0->jupyter-server>=1.24.0->jupyter-server-proxy->cuxfilter-cu12==24.10.*) (4.23.0)\n",
      "Requirement already satisfied: ptyprocess in /usr/local/lib/python3.10/dist-packages (from terminado>=0.8.3->jupyter-server>=1.24.0->jupyter-server-proxy->cuxfilter-cu12==24.10.*) (0.7.0)\n",
      "Requirement already satisfied: argon2-cffi-bindings in /usr/local/lib/python3.10/dist-packages (from argon2-cffi->jupyter-server>=1.24.0->jupyter-server-proxy->cuxfilter-cu12==24.10.*) (21.2.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat>=5.2.0->jupyter-server>=1.24.0->jupyter-server-proxy->cuxfilter-cu12==24.10.*) (2024.10.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat>=5.2.0->jupyter-server>=1.24.0->jupyter-server-proxy->cuxfilter-cu12==24.10.*) (0.35.1)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat>=5.2.0->jupyter-server>=1.24.0->jupyter-server-proxy->cuxfilter-cu12==24.10.*) (0.22.3)\n",
      "Requirement already satisfied: cffi>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from argon2-cffi-bindings->argon2-cffi->jupyter-server>=1.24.0->jupyter-server-proxy->cuxfilter-cu12==24.10.*) (1.17.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->nbconvert>=6.4.4->jupyter-server>=1.24.0->jupyter-server-proxy->cuxfilter-cu12==24.10.*) (2.6)\n",
      "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi->jupyter-server>=1.24.0->jupyter-server-proxy->cuxfilter-cu12==24.10.*) (2.22)\n",
      "Downloading treelite-4.3.0-py3-none-manylinux2014_x86_64.whl (915 kB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 916.0/916.0 kB 27.3 MB/s eta 0:00:00\n",
      "Downloading dask-2024.9.0-py3-none-any.whl (1.3 MB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.3/1.3 MB 47.5 MB/s eta 0:00:00\n",
      "Downloading dask_expr-1.1.14-py3-none-any.whl (242 kB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 242.6/242.6 kB 20.7 MB/s eta 0:00:00\n",
      "Downloading distributed-2024.9.0-py3-none-any.whl (1.0 MB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.0/1.0 MB 49.4 MB/s eta 0:00:00\n",
      "Downloading datashader-0.16.3-py2.py3-none-any.whl (18.3 MB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 18.3/18.3 MB 68.5 MB/s eta 0:00:00\n",
      "Downloading jupyter_server_proxy-4.4.0-py3-none-any.whl (37 kB)\n",
      "Downloading libucx_cu12-1.17.0.post1-py3-none-manylinux_2_28_x86_64.whl (26.9 MB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 26.9/26.9 MB 15.9 MB/s eta 0:00:00\n",
      "Downloading pynvml-11.4.1-py3-none-any.whl (46 kB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 47.0/47.0 kB 3.4 MB/s eta 0:00:00\n",
      "Downloading simpervisor-1.0.0-py3-none-any.whl (8.3 kB)\n",
      "Downloading zict-3.0.0-py2.py3-none-any.whl (43 kB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 43.3/43.3 kB 3.4 MB/s eta 0:00:00\n",
      "Downloading pyct-0.5.0-py2.py3-none-any.whl (15 kB)\n",
      "Downloading sortedcontainers-2.4.0-py2.py3-none-any.whl (29 kB)\n",
      "Downloading tblib-3.0.0-py3-none-any.whl (12 kB)\n",
      "Installing collected packages: sortedcontainers, zict, tblib, simpervisor, pynvml, pyct, libucx-cu12, libcuspatial-cu12, ucx-py-cu12, treelite, libucxx-cu12, dask, cuproj-cu12, ucxx-cu12, distributed, dask-expr, cucim-cu12, rapids-dask-dependency, datashader, cuvs-cu12, distributed-ucxx-cu12, dask-cudf-cu12, dask-cuda, cuspatial-cu12, raft-dask-cu12, cuml-cu12, cugraph-cu12, jupyter-server-proxy, cuxfilter-cu12\n",
      "  Attempting uninstall: pynvml\n",
      "    Found existing installation: pynvml 12.0.0\n",
      "    Uninstalling pynvml-12.0.0:\n",
      "      Successfully uninstalled pynvml-12.0.0\n",
      "  Attempting uninstall: dask\n",
      "    Found existing installation: dask 2024.10.0\n",
      "    Uninstalling dask-2024.10.0:\n",
      "      Successfully uninstalled dask-2024.10.0\n",
      "Successfully installed cucim-cu12-24.10.0 cugraph-cu12-24.10.0 cuml-cu12-24.10.0 cuproj-cu12-24.10.0 cuspatial-cu12-24.10.0 cuvs-cu12-24.10.0 cuxfilter-cu12-24.10.0 dask-2024.9.0 dask-cuda-24.10.0 dask-cudf-cu12-24.10.1 dask-expr-1.1.14 datashader-0.16.3 distributed-2024.9.0 distributed-ucxx-cu12-0.40.0 jupyter-server-proxy-4.4.0 libcuspatial-cu12-24.10.0 libucx-cu12-1.17.0.post1 libucxx-cu12-0.40.0 pyct-0.5.0 pynvml-11.4.1 raft-dask-cu12-24.10.0 rapids-dask-dependency-24.10.0 simpervisor-1.0.0 sortedcontainers-2.4.0 tblib-3.0.0 treelite-4.3.0 ucx-py-cu12-0.40.0 ucxx-cu12-0.40.0 zict-3.0.0\n",
      "\n",
      "        ***********************************************************************\n",
      "        The pip install of RAPIDS is complete.\n",
      "\n",
      "        Please do not run any further installation from the conda based installation methods, as they may cause issues!\n",
      "\n",
      "        Please ensure that you're pulling from the git repo to remain updated with the latest working install scripts.\n",
      "\n",
      "        Troubleshooting:\n",
      "            - If there is an installation failure, please check back on RAPIDSAI owned templates/notebooks to see how to update your personal files.\n",
      "            - If an installation failure persists when using the latest script, please make an issue on https://github.com/rapidsai-community/rapidsai-csp-utils\n",
      "        ***********************************************************************\n",
      "        \n"
     ]
    }
   ],
   "source": [
    "import locale\n",
    "locale.getpreferredencoding = lambda: \"UTF-8\"\n",
    "\n",
    "!git clone https://github.com/rapidsai/rapidsai-csp-utils.git\n",
    "!python rapidsai-csp-utils/colab/pip-install.py  # for this to run, please enable GPU runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1733930439870,
     "user": {
      "displayName": "Rhenald",
      "userId": "16257034015682689428"
     },
     "user_tz": -480
    },
    "id": "FNAqfi037oBj"
   },
   "outputs": [],
   "source": [
    "# !pip install gensim==4.1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1733930439870,
     "user": {
      "displayName": "Rhenald",
      "userId": "16257034015682689428"
     },
     "user_tz": -480
    },
    "id": "PCbkAx2BJia3"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1BCkJVZ6jqGl"
   },
   "source": [
    "### Define Machine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vepTPnO7wFPA"
   },
   "source": [
    "#### Old"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1733930439871,
     "user": {
      "displayName": "Rhenald",
      "userId": "16257034015682689428"
     },
     "user_tz": -480
    },
    "id": "AKSJ9QpWjqGm"
   },
   "outputs": [],
   "source": [
    "# # uses gensim (CPU based) for topic modeling\n",
    "\n",
    "# import pandas as pd\n",
    "# from cuml.feature_extraction.text import TfidfVectorizer\n",
    "# from transformers import pipeline\n",
    "# from sentence_transformers import SentenceTransformer, util\n",
    "# import cupy as cp\n",
    "# import numpy as np\n",
    "# import re\n",
    "# import time\n",
    "# import torch\n",
    "# from tqdm import tqdm\n",
    "# from gensim.models.ldamulticore import LdaMulticore\n",
    "# from gensim.corpora import Dictionary\n",
    "\n",
    "# class ReviewAnalyzer:\n",
    "#     def __init__(self, df, metrics, subset_indices=None, model_name='cardiffnlp/twitter-roberta-base-sentiment'):\n",
    "#         if subset_indices is not None:\n",
    "#             self.df = df.iloc[subset_indices]\n",
    "#         else:\n",
    "#             self.df = df\n",
    "#         # df.reset_index(inplace=True)\n",
    "\n",
    "#         self.reviews = self.df['Combined_Reviews']  # Assuming the review text is in a column named 'Combined_Reviews'\n",
    "#         self.metrics = metrics\n",
    "#         self.vectorizer = TfidfVectorizer(stop_words='english')\n",
    "\n",
    "#         # This is for one GPU usage\n",
    "#         # self.sentiment_pipeline = pipeline(\"sentiment-analysis\", model=model_name, device=0)\n",
    "#         # self.similarity_model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2', device='cuda')\n",
    "\n",
    "#         # This if for multi GPU usage\n",
    "#         # Disable parallelism before tokenization\n",
    "#         os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "#         self.sentiment_pipeline = pipeline(\"sentiment-analysis\", model=model_name, device_map='auto', batch_size=128)\n",
    "#         self.similarity_model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
    "#         # Enable parallelism before tokenization\n",
    "#         os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\"\n",
    "#         self.model_name = model_name\n",
    "\n",
    "#     def preprocess(self):\n",
    "#         self.X = self.vectorizer.fit_transform(self.reviews)\n",
    "#         self.X = cp.array(self.X.toarray(), dtype=cp.float32)  # Convert to dense CuPy array\n",
    "\n",
    "#     def get_topics(self):\n",
    "#         texts = [re.split(r'\\W+', review.lower()) for review in self.reviews]\n",
    "#         dictionary = Dictionary(texts)\n",
    "#         corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "\n",
    "#         # Initialize and train the Gensim LDA model with multicore support\n",
    "#         lda_model = LdaMulticore(corpus, num_topics=len(self.metrics), id2word=dictionary, passes=10, workers=4, random_state=42)\n",
    "\n",
    "#         topics = {}\n",
    "#         for idx, topic in lda_model.print_topics(-1):\n",
    "#             topics[idx] = topic\n",
    "\n",
    "#         return topics\n",
    "\n",
    "#     def analyze_sentiments(self):\n",
    "#         segment_sentiments = []\n",
    "#         for review in tqdm(self.reviews, desc=\"Processing Reviews for Sentiment Analysis\"):\n",
    "#             sentence_segments = re.split(r'(?<=\\.)\\s+(?=[A-Z])', review)\n",
    "#             for segment in sentence_segments:\n",
    "#                 if segment.strip():  # Ignore empty segments\n",
    "#                     try:\n",
    "#                         sentiment = self.sentiment_pipeline(segment.strip())[0]\n",
    "#                         sentiment_score = -sentiment['score'] if sentiment['label'] == 'LABEL_0' else sentiment['score']\n",
    "#                         segment_sentiments.append((segment.strip(), sentiment['label'], sentiment_score))\n",
    "#                     except Exception as e:\n",
    "#                         pass\n",
    "#                         # print(f\"Error processing segment: {segment.strip()}\")\n",
    "#                         # print(f\"Error message: {e}\")\n",
    "\n",
    "#         return segment_sentiments\n",
    "\n",
    "#     def analyze_topics(self):\n",
    "#         segment_topics = []\n",
    "#         texts = [re.split(r'\\W+', review.lower()) for review in self.reviews]\n",
    "#         dictionary = Dictionary(texts)\n",
    "#         corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "\n",
    "#         # Initialize and train the Gensim LDA model with multicore support\n",
    "#         lda_model = LdaMulticore(corpus, num_topics=len(self.metrics), id2word=dictionary, passes=10, workers=4, random_state=42)\n",
    "\n",
    "#         for review in tqdm(self.reviews, desc=\"Processing Reviews for Topic Modeling\"):\n",
    "#             sentence_segments = re.split(r'(?<=\\.)\\s+(?=[A-Z])', review)\n",
    "#             for segment in sentence_segments:\n",
    "#                 if segment.strip():  # Ignore empty segments\n",
    "#                     try:\n",
    "#                         segment_text = re.split(r'\\W+', segment.lower())\n",
    "#                         segment_bow = dictionary.doc2bow(segment_text)\n",
    "#                         segment_topic_dist = lda_model[segment_bow]\n",
    "#                         segment_topic = max(segment_topic_dist, key=lambda x: x[1])[0]\n",
    "#                         segment_topics.append((segment.strip(), segment_topic))\n",
    "#                     except Exception as e:\n",
    "#                         pass\n",
    "#                         # print(f\"Error processing segment: {segment.strip()}\")\n",
    "#                         # print(f\"Error message: {e}\")\n",
    "\n",
    "#         return segment_topics\n",
    "\n",
    "#     def map_segments_to_metrics(self, segment_sentiments):\n",
    "#         metric_embeddings = self.similarity_model.encode(self.metrics, convert_to_tensor=True)\n",
    "#         segment_texts = [segment for segment, _, _ in segment_sentiments]\n",
    "#         segment_embeddings = self.similarity_model.encode(segment_texts, convert_to_tensor=True)\n",
    "\n",
    "#         similarity_scores = util.pytorch_cos_sim(segment_embeddings, metric_embeddings)\n",
    "\n",
    "#         segment_to_metric = {}\n",
    "#         for i, segment in enumerate(segment_sentiments):\n",
    "#             segment_text, sentiment_label, sentiment_score = segment\n",
    "#             most_similar_metric_idx = torch.argmax(similarity_scores[i])  # Use GPU for argmax\n",
    "#             most_similar_metric = self.metrics[most_similar_metric_idx.item()]  # Convert to Python int\n",
    "#             segment_to_metric[segment_text] = (most_similar_metric, sentiment_label, sentiment_score)\n",
    "\n",
    "#         return segment_to_metric\n",
    "\n",
    "#     def analyze_reviews(self):\n",
    "#         self.preprocess()\n",
    "#         topics = self.get_topics()\n",
    "#         # print(\"Topics and their top words:\", topics)\n",
    "\n",
    "#         segment_sentiments = self.analyze_sentiments()\n",
    "#         # print(\"Segment sentiments and their topics:\", segment_sentiments)\n",
    "\n",
    "#         segment_to_metric = self.map_segments_to_metrics(segment_sentiments)\n",
    "#         # print(\"Segment to metric mapping:\", segment_to_metric)\n",
    "\n",
    "#         # Convert to DataFrame\n",
    "#         df_data = []\n",
    "#         for review, glassdoor_id in zip(self.reviews, self.df['index']):\n",
    "#             sentence_segments = re.split(r'(?<=\\.)\\s+(?=[A-Z])', review)\n",
    "#             for segment in sentence_segments:\n",
    "#                 if segment.strip():  # Ignore empty segments\n",
    "#                     if segment.strip() in segment_to_metric:\n",
    "#                         metric, sentiment_label, sentiment_score = segment_to_metric[segment.strip()]\n",
    "#                         df_data.append([review, segment.strip(), metric, sentiment_label, sentiment_score, glassdoor_id])\n",
    "#                     else:\n",
    "#                         pass\n",
    "#                         # print(f\"Segment not found in mapping: {segment.strip()}\")\n",
    "\n",
    "#         df_reviews = pd.DataFrame(df_data, columns=['Review', 'Segment', 'Metric', 'Sentiment Label', 'Sentiment Score', 'index'])\n",
    "\n",
    "#         # Calculate average sentiment score for each metric\n",
    "#         metric_summary = df_reviews.groupby('Metric')['Sentiment Score'].mean().reset_index()\n",
    "\n",
    "#         return df_reviews, metric_summary\n",
    "\n",
    "# # Example usage:\n",
    "# # analyzer = ReviewAnalyzer(df, metrics)\n",
    "# # df_reviews, metric_summary = analyzer.analyze_reviews()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1733930439871,
     "user": {
      "displayName": "Rhenald",
      "userId": "16257034015682689428"
     },
     "user_tz": -480
    },
    "id": "2tleZAcD0Xl8"
   },
   "outputs": [],
   "source": [
    "# # new and improved\n",
    "\n",
    "# import pandas as pd\n",
    "# from cuml.feature_extraction.text import TfidfVectorizer\n",
    "# from transformers import pipeline\n",
    "# from sentence_transformers import SentenceTransformer, util\n",
    "# import cupy as cp\n",
    "# import numpy as np\n",
    "# import re\n",
    "# import torch\n",
    "# from tqdm import tqdm\n",
    "# from gensim.models.ldamulticore import LdaMulticore\n",
    "# from gensim.corpora import Dictionary\n",
    "\n",
    "# class ReviewAnalyzer:\n",
    "#     def __init__(self, df, metrics, subset_indices=None, model_name='cardiffnlp/twitter-roberta-base-sentiment'):\n",
    "#         if subset_indices is not None:\n",
    "#             self.df = df.iloc[subset_indices]\n",
    "#         else:\n",
    "#             self.df = df\n",
    "\n",
    "#         self.reviews = self.df['Combined_Reviews']\n",
    "#         self.metrics = metrics\n",
    "#         self.vectorizer = TfidfVectorizer(stop_words='english')\n",
    "\n",
    "#         # Disable parallelism before tokenization\n",
    "#         os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "#         self.sentiment_pipeline = pipeline(\"sentiment-analysis\", model=model_name, device=0, batch_size=128)\n",
    "#         self.similarity_model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2', device='cuda')\n",
    "#         # Enable parallelism after tokenization\n",
    "#         os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\"\n",
    "#         self.model_name = model_name\n",
    "\n",
    "#     def preprocess(self):\n",
    "#         self.X = self.vectorizer.fit_transform(self.reviews)\n",
    "#         self.X = cp.array(self.X.toarray(), dtype=cp.float32)  # Convert to dense CuPy array\n",
    "\n",
    "#     def get_topics(self):\n",
    "#         texts = [re.split(r'\\W+', review.lower()) for review in self.reviews]\n",
    "#         dictionary = Dictionary(texts)\n",
    "#         corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "\n",
    "#         # Initialize and train the Gensim LDA model with multicore support\n",
    "#         lda_model = LdaMulticore(corpus, num_topics=len(self.metrics), id2word=dictionary, passes=10, workers=4, random_state=42)\n",
    "\n",
    "#         topics = {}\n",
    "#         for idx, topic in lda_model.print_topics(-1):\n",
    "#             topics[idx] = topic\n",
    "\n",
    "#         return topics\n",
    "\n",
    "#     def analyze_sentiments(self):\n",
    "#         segment_sentiments = []\n",
    "#         batch_size = 128  # Adjust batch size based on your GPU memory\n",
    "#         for i in tqdm(range(0, len(self.reviews), batch_size), desc=\"Processing Reviews for Sentiment Analysis\"):\n",
    "#             batch_reviews = self.reviews[i:i+batch_size]\n",
    "#             batch_segments = []\n",
    "#             for review in batch_reviews:\n",
    "#                 sentence_segments = re.split(r'(?<=\\.)\\s+(?=[A-Z])', review)\n",
    "#                 batch_segments.extend([segment.strip() for segment in sentence_segments if segment.strip()])\n",
    "\n",
    "#             if batch_segments:\n",
    "#                 try:\n",
    "#                     sentiments = self.sentiment_pipeline(batch_segments)\n",
    "#                     for segment, sentiment in zip(batch_segments, sentiments):\n",
    "#                         sentiment_score = -sentiment['score'] if sentiment['label'] == 'LABEL_0' else sentiment['score']\n",
    "#                         segment_sentiments.append((segment, sentiment['label'], sentiment_score))\n",
    "#                 except Exception as e:\n",
    "#                     pass\n",
    "#                     # Handle exceptions if needed\n",
    "#         return segment_sentiments\n",
    "\n",
    "#     def analyze_topics(self):\n",
    "#         segment_topics = []\n",
    "#         texts = [re.split(r'\\W+', review.lower()) for review in self.reviews]\n",
    "#         dictionary = Dictionary(texts)\n",
    "#         corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "\n",
    "#         # Initialize and train the Gensim LDA model with multicore support\n",
    "#         lda_model = LdaMulticore(corpus, num_topics=len(self.metrics), id2word=dictionary, passes=10, workers=4, random_state=42)\n",
    "\n",
    "#         for review in tqdm(self.reviews, desc=\"Processing Reviews for Topic Modeling\"):\n",
    "#             sentence_segments = re.split(r'(?<=\\.)\\s+(?=[A-Z])', review)\n",
    "#             for segment in sentence_segments:\n",
    "#                 if segment.strip():\n",
    "#                     try:\n",
    "#                         segment_text = re.split(r'\\W+', segment.lower())\n",
    "#                         segment_bow = dictionary.doc2bow(segment_text)\n",
    "#                         segment_topic_dist = lda_model[segment_bow]\n",
    "#                         segment_topic = max(segment_topic_dist, key=lambda x: x[1])[0]\n",
    "#                         segment_topics.append((segment.strip(), segment_topic))\n",
    "#                     except Exception as e:\n",
    "#                         pass\n",
    "#                         # Handle exceptions if needed\n",
    "#         return segment_topics\n",
    "\n",
    "#     def map_segments_to_metrics(self, segment_sentiments):\n",
    "#         metric_embeddings = self.similarity_model.encode(self.metrics, convert_to_tensor=True)\n",
    "#         segment_texts = [segment for segment, _, _ in segment_sentiments]\n",
    "#         segment_embeddings = self.similarity_model.encode(segment_texts, convert_to_tensor=True)\n",
    "\n",
    "#         similarity_scores = util.pytorch_cos_sim(segment_embeddings, metric_embeddings)\n",
    "\n",
    "#         segment_to_metric = {}\n",
    "#         for i, segment in enumerate(segment_sentiments):\n",
    "#             segment_text, sentiment_label, sentiment_score = segment\n",
    "#             most_similar_metric_idx = torch.argmax(similarity_scores[i])\n",
    "#             most_similar_metric = self.metrics[most_similar_metric_idx.item()]\n",
    "#             segment_to_metric[segment_text] = (most_similar_metric, sentiment_label, sentiment_score)\n",
    "\n",
    "#         return segment_to_metric\n",
    "\n",
    "#     def analyze_reviews(self):\n",
    "#         self.preprocess()\n",
    "#         topics = self.get_topics()\n",
    "\n",
    "#         segment_sentiments = self.analyze_sentiments()\n",
    "\n",
    "#         segment_to_metric = self.map_segments_to_metrics(segment_sentiments)\n",
    "\n",
    "#         df_data = []\n",
    "#         for review, glassdoor_id in zip(self.reviews, self.df['index']):\n",
    "#             sentence_segments = re.split(r'(?<=\\.)\\s+(?=[A-Z])', review)\n",
    "#             for segment in sentence_segments:\n",
    "#                 if segment.strip():\n",
    "#                     if segment.strip() in segment_to_metric:\n",
    "#                         metric, sentiment_label, sentiment_score = segment_to_metric[segment.strip()]\n",
    "#                         df_data.append([review, segment.strip(), metric, sentiment_label, sentiment_score, glassdoor_id])\n",
    "#                     else:\n",
    "#                         pass\n",
    "\n",
    "#         df_reviews = pd.DataFrame(df_data, columns=['Review', 'Segment', 'Metric', 'Sentiment Label', 'Sentiment Score', 'index'])\n",
    "\n",
    "#         metric_summary = df_reviews.groupby('Metric')['Sentiment Score'].mean().reset_index()\n",
    "\n",
    "#         return df_reviews, metric_summary\n",
    "\n",
    "# # Example usage:\n",
    "# # analyzer = ReviewAnalyzer(df, metrics)\n",
    "# # df_reviews, metric_summary = analyzer.analyze_reviews()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1733930439871,
     "user": {
      "displayName": "Rhenald",
      "userId": "16257034015682689428"
     },
     "user_tz": -480
    },
    "id": "pwKvuJX37z7u"
   },
   "outputs": [],
   "source": [
    "# # single initialization\n",
    "\n",
    "# import pandas as pd\n",
    "# from cuml.feature_extraction.text import TfidfVectorizer\n",
    "# from transformers import pipeline\n",
    "# from sentence_transformers import SentenceTransformer, util\n",
    "# import cupy as cp\n",
    "# import numpy as np\n",
    "# import re\n",
    "# import torch\n",
    "# from tqdm import tqdm\n",
    "# from gensim.models.ldamulticore import LdaMulticore\n",
    "# from gensim.corpora import Dictionary\n",
    "# import os\n",
    "\n",
    "# class ReviewAnalyzer:\n",
    "#     def __init__(self, df, metrics, subset_indices=None, model_name='cardiffnlp/twitter-roberta-base-sentiment'):\n",
    "#         if subset_indices is not None:\n",
    "#             self.df = df.iloc[subset_indices]\n",
    "#         else:\n",
    "#             self.df = df\n",
    "\n",
    "#         self.reviews = self.df['Combined_Reviews']\n",
    "#         self.metrics = metrics\n",
    "#         self.vectorizer = TfidfVectorizer(stop_words='english')\n",
    "\n",
    "#         # Disable parallelism before tokenization\n",
    "#         os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "#         self.sentiment_pipeline = pipeline(\"sentiment-analysis\", model=model_name, device=0, batch_size=128)\n",
    "#         self.similarity_model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2', device='cuda')\n",
    "#         # Enable parallelism after tokenization\n",
    "#         os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\"\n",
    "#         self.model_name = model_name\n",
    "\n",
    "#     def preprocess(self):\n",
    "#         self.X = self.vectorizer.fit_transform(self.reviews)\n",
    "#         self.X = cp.array(self.X.toarray(), dtype=cp.float32)  # Convert to dense CuPy array\n",
    "\n",
    "#     def get_topics(self):\n",
    "#         texts = [re.split(r'\\W+', review.lower()) for review in self.reviews]\n",
    "#         dictionary = Dictionary(texts)\n",
    "#         corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "\n",
    "#         # Initialize and train the Gensim LDA model with multicore support\n",
    "#         lda_model = LdaMulticore(corpus, num_topics=len(self.metrics), id2word=dictionary, passes=10, workers=4, random_state=42)\n",
    "\n",
    "#         topics = {}\n",
    "#         for idx, topic in lda_model.print_topics(-1):\n",
    "#             topics[idx] = topic\n",
    "\n",
    "#         return topics\n",
    "\n",
    "#     def analyze_sentiments(self):\n",
    "#         segment_sentiments = []\n",
    "#         batch_size = 128  # Adjust batch size based on your GPU memory\n",
    "#         for i in tqdm(range(0, len(self.reviews), batch_size), desc=\"Processing Reviews for Sentiment Analysis\"):\n",
    "#             batch_reviews = self.reviews[i:i+batch_size]\n",
    "#             batch_segments = []\n",
    "#             for review in batch_reviews:\n",
    "#                 sentence_segments = re.split(r'(?<=\\.)\\s+(?=[A-Z])', review)\n",
    "#                 batch_segments.extend([segment.strip() for segment in sentence_segments if segment.strip()])\n",
    "\n",
    "#             if batch_segments:\n",
    "#                 try:\n",
    "#                     sentiments = self.sentiment_pipeline(batch_segments)\n",
    "#                     for segment, sentiment in zip(batch_segments, sentiments):\n",
    "#                         sentiment_score = -sentiment['score'] if sentiment['label'] == 'LABEL_0' else sentiment['score']\n",
    "#                         segment_sentiments.append((segment, sentiment['label'], sentiment_score))\n",
    "#                 except Exception as e:\n",
    "#                     pass\n",
    "#                     # Handle exceptions if needed\n",
    "#         return segment_sentiments\n",
    "\n",
    "#     def analyze_topics(self):\n",
    "#         segment_topics = []\n",
    "#         texts = [re.split(r'\\W+', review.lower()) for review in self.reviews]\n",
    "#         dictionary = Dictionary(texts)\n",
    "#         corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "\n",
    "#         # Initialize and train the Gensim LDA model with multicore support\n",
    "#         lda_model = LdaMulticore(corpus, num_topics=len(self.metrics), id2word=dictionary, passes=10, workers=4, random_state=42)\n",
    "\n",
    "#         for review in tqdm(self.reviews, desc=\"Processing Reviews for Topic Modeling\"):\n",
    "#             sentence_segments = re.split(r'(?<=\\.)\\s+(?=[A-Z])', review)\n",
    "#             for segment in sentence_segments:\n",
    "#                 if segment.strip():\n",
    "#                     try:\n",
    "#                         segment_text = re.split(r'\\W+', segment.lower())\n",
    "#                         segment_bow = dictionary.doc2bow(segment_text)\n",
    "#                         segment_topic_dist = lda_model[segment_bow]\n",
    "#                         segment_topic = max(segment_topic_dist, key=lambda x: x[1])[0]\n",
    "#                         segment_topics.append((segment.strip(), segment_topic))\n",
    "#                     except Exception as e:\n",
    "#                         pass\n",
    "#                         # Handle exceptions if needed\n",
    "#         return segment_topics\n",
    "\n",
    "#     def map_segments_to_metrics(self, segment_sentiments):\n",
    "#         metric_embeddings = self.similarity_model.encode(self.metrics, convert_to_tensor=True)\n",
    "#         segment_texts = [segment for segment, _, _ in segment_sentiments]\n",
    "#         segment_embeddings = self.similarity_model.encode(segment_texts, convert_to_tensor=True)\n",
    "\n",
    "#         similarity_scores = util.pytorch_cos_sim(segment_embeddings, metric_embeddings)\n",
    "\n",
    "#         segment_to_metric = {}\n",
    "#         for i, segment in enumerate(segment_sentiments):\n",
    "#             segment_text, sentiment_label, sentiment_score = segment\n",
    "#             most_similar_metric_idx = torch.argmax(similarity_scores[i])\n",
    "#             most_similar_metric = self.metrics[most_similar_metric_idx.item()]\n",
    "#             segment_to_metric[segment_text] = (most_similar_metric, sentiment_label, sentiment_score)\n",
    "\n",
    "#         return segment_to_metric\n",
    "\n",
    "#     def analyze_reviews(self):\n",
    "#         self.preprocess()\n",
    "#         topics = self.get_topics()\n",
    "\n",
    "#         segment_sentiments = self.analyze_sentiments()\n",
    "\n",
    "#         segment_to_metric = self.map_segments_to_metrics(segment_sentiments)\n",
    "\n",
    "#         df_data = []\n",
    "#         for review, glassdoor_id in zip(self.reviews, self.df['index']):\n",
    "#             sentence_segments = re.split(r'(?<=\\.)\\s+(?=[A-Z])', review)\n",
    "#             for segment in sentence_segments:\n",
    "#                 if segment.strip():\n",
    "#                     if segment.strip() in segment_to_metric:\n",
    "#                         metric, sentiment_label, sentiment_score = segment_to_metric[segment.strip()]\n",
    "#                         df_data.append([review, segment.strip(), metric, sentiment_label, sentiment_score, glassdoor_id])\n",
    "#                     else:\n",
    "#                         pass\n",
    "\n",
    "#         df_reviews = pd.DataFrame(df_data, columns=['Review', 'Segment', 'Metric', 'Sentiment Label', 'Sentiment Score', 'index'])\n",
    "\n",
    "#         metric_summary = df_reviews.groupby('Metric')['Sentiment Score'].mean().reset_index()\n",
    "\n",
    "#         return df_reviews, metric_summary\n",
    "\n",
    "# # Example usage:\n",
    "# # analyzer = ReviewAnalyzer(df, metrics)\n",
    "# # df_reviews, metric_summary = analyzer.analyze_reviews()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1733930439871,
     "user": {
      "displayName": "Rhenald",
      "userId": "16257034015682689428"
     },
     "user_tz": -480
    },
    "id": "Jro_nsZ-9Kh_"
   },
   "outputs": [],
   "source": [
    "# # single initialization\n",
    "\n",
    "# import pandas as pd\n",
    "# from cuml.feature_extraction.text import TfidfVectorizer\n",
    "# from transformers import pipeline\n",
    "# from sentence_transformers import SentenceTransformer, util\n",
    "# import cupy as cp\n",
    "# import numpy as np\n",
    "# import re\n",
    "# import torch\n",
    "# from tqdm import tqdm\n",
    "# import os\n",
    "# from sklearn.decomposition import LatentDirichletAllocation\n",
    "# from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# class ReviewAnalyzer:\n",
    "#     def __init__(self, df, metrics, subset_indices=None, model_name='cardiffnlp/twitter-roberta-base-sentiment'):\n",
    "#         if subset_indices is not None:\n",
    "#             self.df = df.iloc[subset_indices]\n",
    "#         else:\n",
    "#             self.df = df\n",
    "\n",
    "#         self.reviews = self.df['Combined_Reviews']\n",
    "#         self.metrics = metrics\n",
    "#         self.vectorizer = TfidfVectorizer(stop_words='english')\n",
    "\n",
    "#         # Disable parallelism before tokenization\n",
    "#         os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "#         self.sentiment_pipeline = pipeline(\"sentiment-analysis\", model=model_name, device=0, batch_size=128)\n",
    "#         self.similarity_model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2', device='cuda')\n",
    "#         # Enable parallelism after tokenization\n",
    "#         os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\"\n",
    "#         self.model_name = model_name\n",
    "\n",
    "#     def preprocess(self):\n",
    "#         self.X = self.vectorizer.fit_transform(self.reviews)\n",
    "#         self.X = cp.array(self.X.toarray(), dtype=cp.float32)  # Convert to dense CuPy array\n",
    "\n",
    "#     def get_topics(self):\n",
    "#         texts = [re.split(r'\\W+', review.lower()) for review in self.reviews]\n",
    "#         texts = [' '.join(text) for text in texts]\n",
    "#         vectorizer = CountVectorizer(stop_words='english')\n",
    "#         X = vectorizer.fit_transform(texts)\n",
    "\n",
    "#         lda_model = LatentDirichletAllocation(n_components=len(self.metrics), random_state=42, n_jobs=-1)\n",
    "#         lda_model.fit(X)\n",
    "\n",
    "#         topics = {}\n",
    "#         for idx, topic in enumerate(lda_model.components_):\n",
    "#             topics[idx] = ' '.join([vectorizer.get_feature_names_out()[i] for i in topic.argsort()[:-11:-1]])\n",
    "\n",
    "#         return topics\n",
    "\n",
    "#     def analyze_sentiments(self):\n",
    "#         segment_sentiments = []\n",
    "#         batch_size = 128  # Adjust batch size based on your GPU memory\n",
    "#         for i in tqdm(range(0, len(self.reviews), batch_size), desc=\"Processing Reviews for Sentiment Analysis\"):\n",
    "#             batch_reviews = self.reviews[i:i+batch_size]\n",
    "#             batch_segments = []\n",
    "#             for review in batch_reviews:\n",
    "#                 sentence_segments = re.split(r'(?<=\\.)\\s+(?=[A-Z])', review)\n",
    "#                 batch_segments.extend([segment.strip() for segment in sentence_segments if segment.strip()])\n",
    "\n",
    "#             if batch_segments:\n",
    "#                 try:\n",
    "#                     sentiments = self.sentiment_pipeline(batch_segments)\n",
    "#                     for segment, sentiment in zip(batch_segments, sentiments):\n",
    "#                         sentiment_score = -sentiment['score'] if sentiment['label'] == 'LABEL_0' else sentiment['score']\n",
    "#                         segment_sentiments.append((segment, sentiment['label'], sentiment_score))\n",
    "#                 except Exception as e:\n",
    "#                     pass\n",
    "#                     # Handle exceptions if needed\n",
    "#         return segment_sentiments\n",
    "\n",
    "#     def analyze_topics(self):\n",
    "#         segment_topics = []\n",
    "#         texts = [re.split(r'\\W+', review.lower()) for review in self.reviews]\n",
    "#         texts = [' '.join(text) for text in texts]\n",
    "#         vectorizer = CountVectorizer(stop_words='english')\n",
    "#         X = vectorizer.fit_transform(texts)\n",
    "\n",
    "#         lda_model = LatentDirichletAllocation(n_components=len(self.metrics), random_state=42, n_jobs=-1)\n",
    "#         lda_model.fit(X)\n",
    "\n",
    "#         for review in tqdm(self.reviews, desc=\"Processing Reviews for Topic Modeling\"):\n",
    "#             sentence_segments = re.split(r'(?<=\\.)\\s+(?=[A-Z])', review)\n",
    "#             for segment in sentence_segments:\n",
    "#                 if segment.strip():\n",
    "#                     try:\n",
    "#                         segment_text = ' '.join(re.split(r'\\W+', segment.lower()))\n",
    "#                         segment_vector = vectorizer.transform([segment_text])\n",
    "#                         segment_topic_dist = lda_model.transform(segment_vector)\n",
    "#                         segment_topic = segment_topic_dist.argmax()\n",
    "#                         segment_topics.append((segment.strip(), segment_topic))\n",
    "#                     except Exception as e:\n",
    "#                         pass\n",
    "#                         # Handle exceptions if needed\n",
    "#         return segment_topics\n",
    "\n",
    "#     def map_segments_to_metrics(self, segment_sentiments):\n",
    "#         metric_embeddings = self.similarity_model.encode(self.metrics, convert_to_tensor=True)\n",
    "#         segment_texts = [segment for segment, _, _ in segment_sentiments]\n",
    "#         segment_embeddings = self.similarity_model.encode(segment_texts, convert_to_tensor=True)\n",
    "\n",
    "#         similarity_scores = util.pytorch_cos_sim(segment_embeddings, metric_embeddings)\n",
    "\n",
    "#         segment_to_metric = {}\n",
    "#         for i, segment in enumerate(segment_sentiments):\n",
    "#             segment_text, sentiment_label, sentiment_score = segment\n",
    "#             most_similar_metric_idx = torch.argmax(similarity_scores[i])\n",
    "#             most_similar_metric = self.metrics[most_similar_metric_idx.item()]\n",
    "#             segment_to_metric[segment_text] = (most_similar_metric, sentiment_label, sentiment_score)\n",
    "\n",
    "#         return segment_to_metric\n",
    "\n",
    "#     def analyze_reviews(self):\n",
    "#         self.preprocess()\n",
    "#         topics = self.get_topics()\n",
    "\n",
    "#         segment_sentiments = self.analyze_sentiments()\n",
    "\n",
    "#         segment_to_metric = self.map_segments_to_metrics(segment_sentiments)\n",
    "\n",
    "#         df_data = []\n",
    "#         for review, glassdoor_id in zip(self.reviews, self.df['index']):\n",
    "#             sentence_segments = re.split(r'(?<=\\.)\\s+(?=[A-Z])', review)\n",
    "#             for segment in sentence_segments:\n",
    "#                 if segment.strip():\n",
    "#                     if segment.strip() in segment_to_metric:\n",
    "#                         metric, sentiment_label, sentiment_score = segment_to_metric[segment.strip()]\n",
    "#                         df_data.append([review, segment.strip(), metric, sentiment_label, sentiment_score, glassdoor_id])\n",
    "#                     else:\n",
    "#                         pass\n",
    "\n",
    "#         df_reviews = pd.DataFrame(df_data, columns=['Review', 'Segment', 'Metric', 'Sentiment Label', 'Sentiment Score', 'index'])\n",
    "\n",
    "#         metric_summary = df_reviews.groupby('Metric')['Sentiment Score'].mean().reset_index()\n",
    "\n",
    "#         return df_reviews, metric_summary\n",
    "\n",
    "# # Example usage:\n",
    "# # analyzer = ReviewAnalyzer(df, metrics)\n",
    "# # df_reviews, metric_summary = analyzer.analyze_reviews()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "znTFWdMXwJua"
   },
   "source": [
    "#### In-use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "executionInfo": {
     "elapsed": 44676,
     "status": "ok",
     "timestamp": 1733930484536,
     "user": {
      "displayName": "Rhenald",
      "userId": "16257034015682689428"
     },
     "user_tz": -480
    },
    "id": "jsGgqY4Fslen"
   },
   "outputs": [],
   "source": [
    "# run on either cpu or gpu or tpu\n",
    "\n",
    "import pandas as pd\n",
    "from transformers import pipeline\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import numpy as np\n",
    "import re\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import tensorflow as tf\n",
    "\n",
    "# Try to import GPU-specific libraries\n",
    "try:\n",
    "    import cupy as cp\n",
    "    from cuml.feature_extraction.text import TfidfVectorizer as CuTfidfVectorizer\n",
    "    gpu_available = True\n",
    "except ImportError:\n",
    "    from sklearn.feature_extraction.text import TfidfVectorizer as SkTfidfVectorizer\n",
    "    gpu_available = False\n",
    "\n",
    "def is_cuda_available():\n",
    "    return torch.cuda.is_available()\n",
    "\n",
    "def is_tpu_available():\n",
    "    try:\n",
    "        tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
    "        return True\n",
    "    except ValueError:\n",
    "        return False\n",
    "\n",
    "class ReviewAnalyzer:\n",
    "    def __init__(self, df, metrics, subset_indices=None, model_name='cardiffnlp/twitter-roberta-base-sentiment'):\n",
    "        if subset_indices is not None:\n",
    "            self.df = df.iloc[subset_indices]\n",
    "        else:\n",
    "            self.df = df\n",
    "\n",
    "        self.reviews = self.df['Combined_Reviews']\n",
    "        self.metrics = metrics\n",
    "\n",
    "        # Choose the appropriate vectorizer based on GPU availability\n",
    "        if gpu_available and is_cuda_available():\n",
    "            self.vectorizer = CuTfidfVectorizer(stop_words='english')\n",
    "        else:\n",
    "            self.vectorizer = SkTfidfVectorizer(stop_words='english')\n",
    "\n",
    "        # Disable parallelism before tokenization\n",
    "        os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "        # Check for TPU availability\n",
    "        if is_tpu_available():\n",
    "            resolver = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
    "            tf.config.experimental_connect_to_cluster(resolver)\n",
    "            tf.tpu.experimental.initialize_tpu_system(resolver)\n",
    "            strategy = tf.distribute.TPUStrategy(resolver)\n",
    "            device = 'tpu'\n",
    "            self.similarity_model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2', device='cpu')\n",
    "        elif is_cuda_available():\n",
    "            device = 0\n",
    "            self.similarity_model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2', device='cuda')\n",
    "        else:\n",
    "            device = -1\n",
    "            self.similarity_model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2', device='cpu')\n",
    "\n",
    "        self.sentiment_pipeline = pipeline(\"sentiment-analysis\", model=model_name, device=device, batch_size=128)\n",
    "\n",
    "        # Enable parallelism after tokenization\n",
    "        os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\"\n",
    "        self.model_name = model_name\n",
    "\n",
    "    def preprocess(self):\n",
    "        self.X = self.vectorizer.fit_transform(self.reviews)\n",
    "        if gpu_available and is_cuda_available():\n",
    "            self.X = cp.array(self.X.toarray(), dtype=cp.float32)  # Convert to dense CuPy array\n",
    "        else:\n",
    "            self.X = np.array(self.X.toarray(), dtype=np.float32)  # Convert to dense NumPy array\n",
    "\n",
    "    def get_topics(self):\n",
    "        texts = [re.split(r'\\W+', review.lower()) for review in self.reviews]\n",
    "        texts = [' '.join(text) for text in texts]\n",
    "        vectorizer = CountVectorizer(stop_words='english')\n",
    "        X = vectorizer.fit_transform(texts)\n",
    "\n",
    "        lda_model = LatentDirichletAllocation(n_components=len(self.metrics), random_state=42, n_jobs=-1)\n",
    "        lda_model.fit(X)\n",
    "\n",
    "        topics = {}\n",
    "        for idx, topic in enumerate(lda_model.components_):\n",
    "            topics[idx] = ' '.join([vectorizer.get_feature_names_out()[i] for i in topic.argsort()[:-11:-1]])\n",
    "\n",
    "        return topics\n",
    "\n",
    "    def analyze_sentiments(self):\n",
    "        segment_sentiments = []\n",
    "        batch_size = 128  # Adjust batch size based on your GPU memory\n",
    "        for i in tqdm(range(0, len(self.reviews), batch_size), desc=\"Processing Reviews for Sentiment Analysis\"):\n",
    "            batch_reviews = self.reviews[i:i+batch_size]\n",
    "            batch_segments = []\n",
    "            for review in batch_reviews:\n",
    "                sentence_segments = re.split(r'(?<=\\.)\\s+(?=[A-Z])', review)\n",
    "                batch_segments.extend([segment.strip() for segment in sentence_segments if segment.strip()])\n",
    "\n",
    "            if batch_segments:\n",
    "                try:\n",
    "                    sentiments = self.sentiment_pipeline(batch_segments)\n",
    "                    for segment, sentiment in zip(batch_segments, sentiments):\n",
    "                        sentiment_score = -sentiment['score'] if sentiment['label'] == 'LABEL_0' else sentiment['score']\n",
    "                        segment_sentiments.append((segment, sentiment['label'], sentiment_score))\n",
    "                except Exception as e:\n",
    "                    pass\n",
    "                    # Handle exceptions if needed\n",
    "        return segment_sentiments\n",
    "\n",
    "    # def analyze_sentiments(self):\n",
    "    #     segment_sentiments = []\n",
    "    #     batch_size = 128  # Adjust batch size based on your GPU memory\n",
    "    #     for i in tqdm(range(0, len(self.reviews), batch_size), desc=\"Processing Reviews for Sentiment Analysis\"):\n",
    "    #         batch_reviews = self.reviews[i:i+batch_size]\n",
    "    #         batch_segments = []\n",
    "    #         for review in batch_reviews:\n",
    "    #             sentence_segments = re.split(r'(?<=\\.)\\s+(?=[A-Z])', review)\n",
    "    #             batch_segments.extend([segment.strip() for segment in sentence_segments if segment.strip()])\n",
    "\n",
    "    #         if batch_segments:\n",
    "    #             try:\n",
    "    #                 sentiments = self.sentiment_pipeline(batch_segments)\n",
    "    #                 for segment, sentiment in zip(batch_segments, sentiments):\n",
    "    #                     sentiment_score = -sentiment['score'] if sentiment['label'] == 'LABEL_0' else sentiment['score']\n",
    "    #                     segment_sentiments.append((segment, sentiment['label'], sentiment_score))\n",
    "    #             except Exception as e:\n",
    "    #                 print(f\"Error processing batch: {e}\")\n",
    "    #                 # Handle exceptions if needed\n",
    "    #     return segment_sentiments\n",
    "\n",
    "\n",
    "    def analyze_topics(self):\n",
    "        segment_topics = []\n",
    "        texts = [re.split(r'\\W+', review.lower()) for review in self.reviews]\n",
    "        texts = [' '.join(text) for text in texts]\n",
    "        vectorizer = CountVectorizer(stop_words='english')\n",
    "        X = vectorizer.fit_transform(texts)\n",
    "\n",
    "        lda_model = LatentDirichletAllocation(n_components=len(self.metrics), random_state=42, n_jobs=-1)\n",
    "        lda_model.fit(X)\n",
    "\n",
    "        for review in tqdm(self.reviews, desc=\"Processing Reviews for Topic Modeling\"):\n",
    "            sentence_segments = re.split(r'(?<=\\.)\\s+(?=[A-Z])', review)\n",
    "            for segment in sentence_segments:\n",
    "                if segment.strip():\n",
    "                    try:\n",
    "                        segment_text = ' '.join(re.split(r'\\W+', segment.lower()))\n",
    "                        segment_vector = vectorizer.transform([segment_text])\n",
    "                        segment_topic_dist = lda_model.transform(segment_vector)\n",
    "                        segment_topic = segment_topic_dist.argmax()\n",
    "                        segment_topics.append((segment.strip(), segment_topic))\n",
    "                    except Exception as e:\n",
    "                        pass\n",
    "                        # Handle exceptions if needed\n",
    "        return segment_topics\n",
    "\n",
    "    def map_segments_to_metrics(self, segment_sentiments):\n",
    "        metric_embeddings = self.similarity_model.encode(self.metrics, convert_to_tensor=True)\n",
    "        segment_texts = [segment for segment, _, _ in segment_sentiments]\n",
    "        segment_embeddings = self.similarity_model.encode(segment_texts, convert_to_tensor=True)\n",
    "\n",
    "        similarity_scores = util.pytorch_cos_sim(segment_embeddings, metric_embeddings)\n",
    "\n",
    "        segment_to_metric = {}\n",
    "        for i, segment in enumerate(segment_sentiments):\n",
    "            segment_text, sentiment_label, sentiment_score = segment\n",
    "            most_similar_metric_idx = torch.argmax(similarity_scores[i])\n",
    "            most_similar_metric = self.metrics[most_similar_metric_idx.item()]\n",
    "            segment_to_metric[segment_text] = (most_similar_metric, sentiment_label, sentiment_score)\n",
    "\n",
    "        return segment_to_metric\n",
    "\n",
    "    def analyze_reviews(self):\n",
    "        self.preprocess()\n",
    "        topics = self.get_topics()\n",
    "\n",
    "        segment_sentiments = self.analyze_sentiments()\n",
    "\n",
    "        segment_to_metric = self.map_segments_to_metrics(segment_sentiments)\n",
    "\n",
    "        df_data = []\n",
    "        for review, glassdoor_id in zip(self.reviews, self.df['index']):\n",
    "            sentence_segments = re.split(r'(?<=\\.)\\s+(?=[A-Z])', review)\n",
    "            for segment in sentence_segments:\n",
    "                if segment.strip():\n",
    "                    if segment.strip() in segment_to_metric:\n",
    "                        metric, sentiment_label, sentiment_score = segment_to_metric[segment.strip()]\n",
    "                        df_data.append([review, segment.strip(), metric, sentiment_label, sentiment_score, glassdoor_id])\n",
    "                    else:\n",
    "                        pass\n",
    "\n",
    "        df_reviews = pd.DataFrame(df_data, columns=['Review', 'Segment', 'Metric', 'Sentiment Label', 'Sentiment Score', 'index'])\n",
    "\n",
    "        metric_summary = df_reviews.groupby('Metric')['Sentiment Score'].mean().reset_index()\n",
    "\n",
    "        return df_reviews, metric_summary\n",
    "\n",
    "# Example usage:\n",
    "# analyzer = ReviewAnalyzer(df, metrics)\n",
    "# df_reviews, metric_summary = analyzer.analyze_reviews()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v1fZKry7lnaM"
   },
   "source": [
    "### Unzip Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 25875,
     "status": "ok",
     "timestamp": 1733930510407,
     "user": {
      "displayName": "Rhenald",
      "userId": "16257034015682689428"
     },
     "user_tz": -480
    },
    "id": "rvmUJ3vilnto",
    "outputId": "515780da-07dd-4cf1-a21f-3ca3f7521ec7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24.530701875686646\n"
     ]
    }
   ],
   "source": [
    "import zipfile\n",
    "import time\n",
    "import os\n",
    "\n",
    "if os.path.isfile(\"text_reviews.json\"):\n",
    "  print(\"File is already unzipped.\")\n",
    "else:\n",
    "  # Specify the path to your .zip file and the extraction directory\n",
    "  zip_file_path = '/content/drive/MyDrive/Colab Notebooks/FINA4359/text_reviews.zip'\n",
    "  extract_to_path = ''\n",
    "\n",
    "  # Open and extract the .zip file\n",
    "  with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
    "      start = time.time()\n",
    "      zip_ref.extractall(extract_to_path)\n",
    "      print(time.time() - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c9QzAQGjjqGn"
   },
   "source": [
    "### Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 53551,
     "status": "ok",
     "timestamp": 1733930563956,
     "user": {
      "displayName": "Rhenald",
      "userId": "16257034015682689428"
     },
     "user_tz": -480
    },
    "id": "6RH-PObIjqGo",
    "outputId": "0bb093bd-fda8-4e9d-9136-555c68bc0ed2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rows: 3431066\n"
     ]
    }
   ],
   "source": [
    "## Load real data\n",
    "\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "try:\n",
    "    df = pd.read_json(\"text_reviews.json\")         ## load in dataset\n",
    "    df.fillna(\"\", inplace=True)\n",
    "\n",
    "    # df1 = pd.read_json(\"text_reviews1.json\")         ## load in dataset\n",
    "    # df1.fillna(\"\", inplace=True)\n",
    "\n",
    "    # df_c = pd.concat([df, df1], ignore_index=True)\n",
    "    # df = df_c\n",
    "    # df.head()\n",
    "\n",
    "except ValueError as e:\n",
    "    print(f\"Error reading JSON file: {e}\")\n",
    "\n",
    "# Combine review columns into one paragraph per group\n",
    "df['Combined_Reviews'] = df[['summary', 'pros', 'cons', 'advice']].agg(' '.join, axis=1)\n",
    "\n",
    "# Drop the original review columns if needed\n",
    "df = df.drop(columns=['summary', 'pros', 'cons', 'advice'])\n",
    "\n",
    "print(f\"rows: {len(df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "alM2722NpYlk"
   },
   "outputs": [],
   "source": [
    "# df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ay9neiU-jqGp"
   },
   "outputs": [],
   "source": [
    "## Set date of data transformation\n",
    "from datetime import date\n",
    "str(date.today())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1PQttv89jqGp"
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "# Get the current time\n",
    "now = datetime.now()\n",
    "\n",
    "# Format the time\n",
    "current_time = now.strftime(\"%H:%M:%S\")\n",
    "print(current_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pTlPSN_ajqGq"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Define the folder path\n",
    "folder_path = f\"/content/drive/MyDrive/Colab Notebooks/FINA4359/output/{date.today()}\"\n",
    "\n",
    "# Check if the folder exists\n",
    "if not os.path.exists(folder_path):\n",
    "    # Create the folder\n",
    "    os.makedirs(folder_path)\n",
    "    print(f\"Folder '{folder_path}' created.\")\n",
    "else:\n",
    "    print(f\"Folder '{folder_path}' already exists.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sujpSXmdI7tP"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JVuXo47cjqGr"
   },
   "source": [
    "### Initiate Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "11S28TwIjqGr",
    "jp-MarkdownHeadingCollapsed": true
   },
   "outputs": [],
   "source": [
    "###### Example usage\n",
    "# reviews = [\n",
    "#     \"Working at XYZ Corp has been a fantastic experience. The team is supportive, and the management genuinely cares about employee growth. I’ve learned so much and will always cherish my time here.\",\n",
    "#     \"My time at ABC Inc. was a mixed bag. While the projects were interesting and challenging, I felt that communication from upper management could have been better. Overall, it was a valuable learning experience.\",\n",
    "#     \"I’ve enjoyed every moment at DEF Ltd. The company culture is inclusive, and there are ample opportunities for professional development. I’m grateful for the mentorship and support I received.\",\n",
    "#     \"GHI Co. has a lot of potential, but there are areas for improvement. The workload can be overwhelming at times, and clearer processes would help. Despite this, I appreciated the collaborative environment.\",\n",
    "#     \"JKL Enterprises is a great place to work. The benefits are excellent, and the work-life balance is respected. I’m leaving with a wealth of knowledge and many fond memories.\",\n",
    "#     \"My experience at MNO Group was generally positive, but there were some challenges. The pace was fast, and sometimes it felt like there wasn’t enough support. However, the team spirit was strong, and I learned a lot.\",\n",
    "#     \"Working at PQR Solutions has been a rewarding journey. The innovative projects and the supportive team made every day exciting. I’m grateful for the opportunities to grow and develop my skills.\",\n",
    "#     \"STU Technologies provided a dynamic work environment. While the fast-paced nature was challenging, it pushed me to improve and adapt quickly. The leadership was inspiring, and I leave with a sense of accomplishment.\",\n",
    "#     \"VWX Innovations was a place of constant learning. The collaborative culture and the emphasis on creativity made it a great place to work. I appreciate the guidance and the friendships I’ve made here.\",\n",
    "#     \"YZA Enterprises had its ups and downs. The workload was heavy, but the team’s camaraderie made it manageable. I’m thankful for the experiences and the professional growth I achieved.\"\n",
    "# ]\n",
    "\n",
    "import gc\n",
    "\n",
    "# Analyze the reviews and print the results (this will simulate the process)\n",
    "u = 257\n",
    "v = 258\n",
    "\n",
    "# loop timer\n",
    "big_start = time.time()\n",
    "\n",
    "df = df.iloc[list(range(u*10000, v*10000))]\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "for c in range(u-u, v-u):\n",
    "    ## Real usage\n",
    "    a = 0 + (c*10000)\n",
    "    b = 10000 + (c*10000)\n",
    "    # a = 0\n",
    "    # b = 10000\n",
    "    print(\"=\"*100)\n",
    "    print(f\"Run {c}: row {a+u*10000} to {b+u*10000}\")\n",
    "    subset_indices = list(range(a, b))\n",
    "\n",
    "\n",
    "    metrics = [\n",
    "    \"Employee Satisfaction\",\n",
    "    \"Communication Effectiveness\",\n",
    "    \"Workload Management\",\n",
    "    \"Professional Development\",\n",
    "    \"Work-Life Balance\",\n",
    "    \"Team Collaboration\",\n",
    "    \"Leadership Quality\",\n",
    "    \"Innovation Encouragement\",\n",
    "    \"Career Advancement Opportunities\",\n",
    "    \"Employee Recognition\",\n",
    "    \"Management Quality\",\n",
    "    \"Benefits and Compensation\",\n",
    "    \"Company Culture\",\n",
    "    \"Job Security\",\n",
    "    \"Productivity\",\n",
    "    \"Quality of Work\",\n",
    "    \"Credibility\",\n",
    "    \"Leadership\"\n",
    "    ]\n",
    "\n",
    "    analyzer = ReviewAnalyzer(df, metrics, subset_indices)\n",
    "\n",
    "    try:\n",
    "        start = time.time()\n",
    "\n",
    "        df_reviews, metric_summary = analyzer.analyze_reviews()\n",
    "        # print(df_reviews.head())\n",
    "        # print(metric_summary.head())\n",
    "\n",
    "        end = time.time()\n",
    "\n",
    "        print(f\" Running Time: {end - start}\")\n",
    "    # except MemoryError as e:\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        try:\n",
    "            print(\"something went wrong\")\n",
    "            df_reviews.to_parquet('output/output_abr.parquet', engine='pyarrow', index=False)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "\n",
    "    ## save output as\n",
    "    # df_reviews.to_parquet(f\"output/{date.today()}/{u+c}_{current_time}.parquet\", engine='pyarrow', index=False)\n",
    "    df_reviews.to_parquet(f\"/content/drive/MyDrive/Colab Notebooks/FINA4359/output/{date.today()}/{u+c}_{current_time}.parquet\", engine='pyarrow', index=False)\n",
    "\n",
    "print(\"=============================\")\n",
    "print(f\"   Big timer: {time.time() - big_start}  \")\n",
    "print(\"=============================\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rGCn9lWWjqGs"
   },
   "outputs": [],
   "source": [
    "# ## sanity check: Read the Parquet file\n",
    "# df_loaded = pd.read_parquet('reviews.parquet', engine='pyarrow')\n",
    "# df_loaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4t9iAugAjqGt"
   },
   "outputs": [],
   "source": [
    "metric_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "25rUq2abjqGt"
   },
   "outputs": [],
   "source": [
    "cp.get_default_memory_pool().free_all_blocks()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XSHrXhpqjqGt"
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S98fM4yajqGt"
   },
   "outputs": [],
   "source": [
    "df_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yKS6c2H0jqGu"
   },
   "outputs": [],
   "source": [
    "df_reviews.to_parquet(f\"/content/drive/MyDrive/Colab Notebooks/FINA4359/output/{date.today()}/{u+c}_{current_time}.parquet\", engine='pyarrow', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RB97_CAi1jxc"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "NdMEHnCzpUUj",
    "bYMsVTvqo98W",
    "vepTPnO7wFPA"
   ],
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
